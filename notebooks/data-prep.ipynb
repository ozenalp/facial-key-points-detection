{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add parent directory to the module search path\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "facialpoints_df = pd.read_csv('../data/KeyFacialPoints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialKeypointsDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_string = self.dataframe.iloc[idx, -1]\n",
    "        image = np.array([int(item) for item in image_string.split()]).reshape(96, 96, 1)\n",
    "        image = image / 255.\n",
    "        keypoints = self.dataframe.iloc[idx, :-1].values.astype('float32')\n",
    "\n",
    "        sample = {'image': image, 'keypoints': keypoints}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, keypoints = sample['image'], sample['keypoints']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1)).copy()  # Added .copy()\n",
    "\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'keypoints': torch.from_numpy(keypoints)}\n",
    "\n",
    "\n",
    "\n",
    "class CustomHorizontalFlip:\n",
    "    def __call__(self, sample):\n",
    "        image, keypoints = sample['image'], sample['keypoints']\n",
    "        image = image[:, ::-1, :]\n",
    "        keypoints = keypoints.copy()\n",
    "        keypoints[::2] = 96. - keypoints[::2]  # Assuming image size is 96x96 and keypoints are scaled accordingly\n",
    "        return {'image': image, 'keypoints': keypoints}\n",
    "\n",
    "\n",
    "class FlipVertical:\n",
    "    def __call__(self, sample):\n",
    "        image, keypoints = sample['image'], sample['keypoints']\n",
    "        image = image[::-1, :, :]\n",
    "        keypoints = keypoints.copy()\n",
    "        keypoints[1::2] = 96. - keypoints[1::2]  # Assuming image size is 96x96 and keypoints are scaled accordingly\n",
    "        return {'image': image, 'keypoints': keypoints}\n",
    "\n",
    "\n",
    "class IncreaseBrightness:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, keypoints = sample['image'], sample['keypoints']\n",
    "        image = np.clip(image + self.value, 0., 1.)  # Ensure values are within [0, 1]\n",
    "        return {'image': image, 'keypoints': keypoints}\n",
    "\n",
    "\n",
    "class DecreaseBrightness:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, keypoints = sample['image'], sample['keypoints']\n",
    "        image = np.clip(image - self.value, 0., 1.)  # Ensure values are within [0, 1]\n",
    "        return {'image': image, 'keypoints': keypoints}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transforms_list = [\n",
    "    transforms.Compose([CustomHorizontalFlip(), ToTensor()]),\n",
    "    transforms.Compose([FlipVertical(), ToTensor()]),\n",
    "    transforms.Compose([IncreaseBrightness(0.1), ToTensor()]),\n",
    "    transforms.Compose([DecreaseBrightness(0.1), ToTensor()])\n",
    "]\n",
    "\n",
    "# Create the original dataset\n",
    "datasets = [FacialKeypointsDataset(dataframe=facialpoints_df, transform=ToTensor())]\n",
    "\n",
    "# Apply each transform to the original data and append to the datasets list\n",
    "for transform in transforms_list:\n",
    "    transformed_dataset = FacialKeypointsDataset(dataframe=facialpoints_df, transform=transform)\n",
    "    datasets.append(transformed_dataset)\n",
    "\n",
    "# Combine original and transformed datasets\n",
    "combined_dataset = ConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set: 7704\n",
      "Length of validation set: 856\n",
      "Length of test set: 2140\n"
     ]
    }
   ],
   "source": [
    "# Determine lengths of splits\n",
    "total_length = len(combined_dataset)\n",
    "train_length = int(0.8 * total_length)  # 80% of the total data\n",
    "test_length = total_length - train_length  # 20% of the total data\n",
    "val_length = int(0.1 * train_length)  # 10% of the training data\n",
    "train_length = train_length - val_length  # Adjust training data length\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = random_split(combined_dataset, [train_length + val_length, test_length])  # First, split into train+val and test\n",
    "train_data, val_data = random_split(train_data, [train_length, val_length])  # Then, split train into train and val\n",
    "\n",
    "# Print length of datasets\n",
    "print(f\"Length of training set: {len(train_data)}\")\n",
    "print(f\"Length of validation set: {len(val_data)}\")\n",
    "print(f\"Length of test set: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training sample: torch.Size([1, 96, 96]), Shape of training label: torch.Size([30])\n",
      "Shape of validation sample: torch.Size([1, 96, 96]), Shape of validation label: torch.Size([30])\n",
      "Shape of test sample: torch.Size([1, 96, 96]), Shape of test label: torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "train_dict = next(iter(train_data))\n",
    "val_dict = next(iter(val_data))\n",
    "test_dict = next(iter(test_data))\n",
    "\n",
    "print(f\"Shape of training sample: {train_dict['image'].shape}, Shape of training label: {train_dict['keypoints'].shape}\")\n",
    "print(f\"Shape of validation sample: {val_dict['image'].shape}, Shape of validation label: {val_dict['keypoints'].shape}\")\n",
    "print(f\"Shape of test sample: {test_dict['image'].shape}, Shape of test label: {test_dict['keypoints'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets\n",
    "torch.save(train_data, '../data/train_data.pth')\n",
    "torch.save(val_data, '../data/val_data.pth')\n",
    "torch.save(test_data, '../data/test_data.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
